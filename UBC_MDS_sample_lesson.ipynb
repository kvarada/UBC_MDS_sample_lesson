{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UBC instructor position sample lesson  \n",
    "\n",
    "### By Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "### Today's plan\n",
    "\n",
    "- Set the stage (~5 mins)\n",
    "- Sample class (~40 mins)\n",
    "- Reflection (~10 mins)\n",
    "- Vision (~10 mins)\n",
    "- Q and A (~10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Set the stage (~5 mins)\n",
    "\n",
    "- I am envisioning this as the second half of the first lesson of DSCI 575 (Advanced Machine Learning in the context of Natural Language Processing (NLP) applications). \n",
    "- It is the first week of the final block of the MDS program's curriculum. \n",
    "- The students have already taken four 1-credit Machine Learning courses: DSCI 571, DSCI 572, DSCI 573, DSCI 563. \n",
    "- They are now ready to apply the concepts they have learned so far on interesting problems. \n",
    "- This course uses Python. \n",
    "- The prerequisites I am assuming are\n",
    "    - Familiarity with dot product, cosine similarity (basic linear algebra)\n",
    "    - Familiarity with neural networks, softmax, which they have done in DSCI 572."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DSCI 575: Advanced Machine Learning (in the context of NLP applications)\n",
    "\n",
    "#### Lecture 1 (Part 2): Introduction to Word Embeddings: _Toronto_ : _UofT_ :: _Vancouver_ : ? \n",
    "Instructor: Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "#### Today's plan\n",
    "- Quick introduction\n",
    "- Word representations\n",
    "    - Sparse representations with co-occurrence matrix\n",
    "    - Dense representation with word2vec (the Skipgram algorithm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "common_texts\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "import sys\n",
    "sys.path.append('code/.')\n",
    "from preprocessing import MyPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN STARTER CODE\n",
    "class CooccurrenceMatrix:\n",
    "    def __init__(self, corpus, \n",
    "                       tokenizer = word_tokenize, \n",
    "                       window_size = 3):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.window_size = window_size\n",
    "        self.vocab = {}\n",
    "        self.cooccurrence_matrix = None    \n",
    "        \n",
    "    def fit_transform(self):\n",
    "        \"\"\"\n",
    "        Creates a co-occurrence matrix. \n",
    "        \n",
    "        Returns vocabulary (dict) and co-occurrence matrix (csr_matrix)\n",
    "        \"\"\"\n",
    "        data=[]\n",
    "        row=[]\n",
    "        col=[]\n",
    "        for tokens in self.corpus:\n",
    "            for target_index, token in enumerate(tokens):\n",
    "                # Get the index of the word in the vocabulary. If the word is not in the vocabulary, \n",
    "                # set the index to the size of the vocabulary. \n",
    "                i = self.vocab.setdefault(token, len(self.vocab))\n",
    "                \n",
    "                # Consider the context words depending upon the context window \n",
    "                start = max(0, target_index - self.window_size)\n",
    "                end = min(len(tokens), target_index + self.window_size + 1)\n",
    "                \n",
    "                for context_index in range(start, end):\n",
    "                    # Do not consider the target word.  \n",
    "                    if target_index == context_index: \n",
    "                        continue                        \n",
    "                    j = self.vocab.setdefault(tokens[context_index], len(self.vocab))\n",
    "                    # Set diagonal to 0\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    data.append(1.0); row.append(i); col.append(j);\n",
    "        self.cooccurrence_matrix = csr_matrix((data,(row,col)))\n",
    "        return self.vocab, self.cooccurrence_matrix\n",
    "            \n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"\n",
    "        Given a word returns the word vector associated with it from the co-occurrence matrix. \n",
    "\n",
    "        Keyword arguments:\n",
    "        word -- (str) the word to look up in the vocab.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # BEGIN SOLUTION\n",
    "        if word in self.vocab: \n",
    "            return self.cooccurrence_matrix[self.vocab[word]]\n",
    "        else:\n",
    "            print('The word not present in the vocab')\n",
    "        # END SOLUTION\n",
    "\n",
    "# END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What should a search engine return when asked the following question? \n",
    "\n",
    "<img src=\"imgs/lexical_ambiguity.png\" width=\"800\" height=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Natural Language Processing (NLP)?\n",
    "\n",
    "<img src=\"imgs/WhatisNLP.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why is it hard?\n",
    "\n",
    "- All the problems related to representation and reasoning in artificial intelligence arise in this domain. \n",
    "- For language understanding, we need a representation that captures its \"meaning\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Today's promise\n",
    "\n",
    "- We will learn a state-of-the art method for word \"meaning\" representation.  \n",
    "\n",
    "#### Specific learning outcomes\n",
    "\n",
    "From this class, you will be able to \n",
    "\n",
    "- Explain the general idea of vector space model.\n",
    "- Explain the difference between sparse and dense word vectors.\n",
    "- Explain the general idea of the skip-gram model.\n",
    "- Use word2vec models to get word similarity and analogies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Representing text\n",
    "\n",
    "- In order to build machine learning models for text, we need to represent it effectively\n",
    "    - How can we represent text that captures its \"meaning\"? \n",
    "- Let's start small. \n",
    "- How can we represent word \"meaning\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning \n",
    "\n",
    "- A favourite topic of philosophers for centuries. \n",
    "- An example from legal domain: [Are hockey gloves gloves or \"articles of plastics\"?](https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258)\n",
    "\n",
    "<blockquote>\n",
    "Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either gloves or as \"articles of plastics.\"\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<img src=\"imgs/hockey_gloves_case.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning: NLP view\n",
    "- Modeling word meaning that allows us to \n",
    "    * draw useful inferences to solve meaning-related problems \n",
    "    * find relationship between words, e.g., which words are similar, which ones have positive or negative connotations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity 1:  Brainstorm ways to represent words (~4 mins) \n",
    "\n",
    "- Suppose you are building a Question Answering system and you are given the following question and three candidate answers. \n",
    "- Discuss the following point with your neighbour(s). \n",
    "    - What kind of relationship between words do we need to capture in order to arrive at the correct answer?  \n",
    "    - Would one-hot representation you have seen before work in this context?\n",
    "    \n",
    "<blockquote>       \n",
    "<p style=\"font-size:30px\"><b>Question:</b> How <b>tall</b> is Machu Picchu?</p>\n",
    "    <p style=\"font-size:30px\"><b>Candidate 1:</b> Machu Picchu is 13.164 degrees south of the equator.</p>    \n",
    "<p style=\"font-size:30px\"><b>Candidate 2:</b> The official height of Machu Picchu is 2,430 m.</p>\n",
    "<p style=\"font-size:30px\"><b>Candidate 3:</b> Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.</p>    \n",
    "</blockquote> \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reminder: One-hot representation\n",
    "\n",
    "- Build the **vocabulary** containing all unique words from the corpus \n",
    "- Represent each word as **one-hot** encoding\n",
    "- A vector of length $V$ such that the value at word index is 1 and all other indices is 0\n",
    "- Example: \n",
    "    * Vocabulary size = 10\n",
    "    * Index of the word *pineapple* = 4\n",
    "    * One-hot vector for *pineapple*:\n",
    "    \\begin{bmatrix} 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 1: Term-term co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributional hypothesis\n",
    "\n",
    "<blockquote> \n",
    "    <p>You shall know a word by the company it keeps.</p>\n",
    "    <footer>Firth, 1957</footer>        \n",
    "</blockquote>\n",
    "\n",
    "<blockquote> \n",
    "If A and B have almost identical environments we say that they are synonyms.\n",
    "<footer>Harris, 1954</footer>    \n",
    "</blockquote>    \n",
    "\n",
    "Example: \n",
    "\n",
    "- Her **child** loves to play in the playground. \n",
    "- Her **kid** loves to play in the playground. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector space model\n",
    "\n",
    "- Model the meaning of a word by placing it into a vector space.  \n",
    "- A standard way to represent meaning in NLP\n",
    "- Distances among words in the vector space indicate the relationship between them. \n",
    "- Called an \"embedding\" because it's embedded into a high-dimensional space\n",
    "\n",
    "<img src=\"imgs/t-SNE_word_embeddings.png\" width=\"700\" height=\"700\">\n",
    "    (Attribution: Jurafsky and Martin 3rd edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-term co-occurrence matrix\n",
    "\n",
    "- The idea is to go through a corpus of text, keeping a count of all of the words that appear in context of each word (within a window).\n",
    "\n",
    "<img src=\"imgs/term-term_comat.png\" width=\"600\" height=\"600\">\n",
    "(Credit: Jurafsky and Martin 3rd edition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing word vectors and similarity \n",
    "\n",
    "<img src=\"imgs/word_vectors_and_angles.png\" width=\"600\" height=\"600\">\n",
    "(Credit: Jurafsky and Martin 3rd edition)\n",
    "\n",
    "- The similarity is calculated using dot products between word vectors.\n",
    "    - Example: $\\vec{\\text{digital}}.\\vec{\\text{information}} = 0 \\times 1 + 1\\times 6 = 6$\n",
    "    - Higher the dot product more similar the words.\n",
    "\n",
    "- We can also calculate a normalized version of dot products. \n",
    "    $$similarity_{cosine}(\\vec{w_1},\\vec{w_2}) = \\frac{\\vec{w_1}.\\vec{w_2}}{\\left\\lVert w_1\\right\\rVert_2 \\left\\lVert w_2\\right\\rVert_2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tall</th>\n",
       "      <th>machu</th>\n",
       "      <th>picchu</th>\n",
       "      <th>13.164</th>\n",
       "      <th>degrees</th>\n",
       "      <th>south</th>\n",
       "      <th>equator</th>\n",
       "      <th>official</th>\n",
       "      <th>height</th>\n",
       "      <th>2,430</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>sea</th>\n",
       "      <th>level</th>\n",
       "      <th>1,000</th>\n",
       "      <th>3,300</th>\n",
       "      <th>ft</th>\n",
       "      <th>lower</th>\n",
       "      <th>elevation</th>\n",
       "      <th>3,400</th>\n",
       "      <th>11,200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tall</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machu</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picchu</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.164</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tall  machu  picchu  13.164  degrees  south  equator  official  \\\n",
       "tall        0      1       1       0        0      0        0         0   \n",
       "machu       1      0       5       1        1      0        0         1   \n",
       "picchu      1      5       0       1        1      1        0         1   \n",
       "13.164      0      1       1       0        1      1        1         0   \n",
       "degrees     0      1       1       1        0      1        1         0   \n",
       "\n",
       "         height  2,430  ...  mean  sea  level  1,000  3,300  ft  lower  \\\n",
       "tall          0      0  ...     0    0      0      0      0   0      0   \n",
       "machu         1      2  ...     0    0      0      0      0   0      0   \n",
       "picchu        1      2  ...     0    0      0      0      0   0      0   \n",
       "13.164        0      0  ...     0    0      0      0      0   0      0   \n",
       "degrees       0      0  ...     0    0      0      0      0   0      0   \n",
       "\n",
       "         elevation  3,400  11,200  \n",
       "tall             0      0       0  \n",
       "machu            0      0       0  \n",
       "picchu           0      0       0  \n",
       "13.164           0      0       0  \n",
       "degrees          0      0       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's build term-term co-occurrence matrix for our text. \n",
    "corpus = [\"How tall is Machu Picchu?\",\n",
    "          \"Machu Picchu is 13.164 degrees south of the equator.\", \n",
    "          \"The official height of Machu Picchu is 2,430 m.\",\n",
    "          \"Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.\",\n",
    "          \"It is 80 kilometres (50 miles) northwest of Cusco, on the crest of the mountain Machu Picchu, located about 2,430 metres (7,970 feet) above mean sea level, over 1,000 metres (3,300 ft) lower than Cusco, which has an elevation of 3,400 metres (11,200 ft).\"\n",
    "         ]\n",
    "pp = MyPreprocessor()\n",
    "pp_corpus = pp.preprocess_corpus(corpus)\n",
    "cm = CooccurrenceMatrix(pp_corpus)\n",
    "vocab, comat = cm.fit_transform()\n",
    "words = [key for key, value in sorted(vocab.items(), \n",
    "                                      key = lambda item: (item[1],item[0]))]\n",
    "df = pd.DataFrame(comat.todense(), \n",
    "                  columns = words, \n",
    "                  index = words,\n",
    "                  dtype = np.int8\n",
    "                 )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def similarity(word1, word2): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vec1 = cm.get_word_vector(word1).todense().flatten()\n",
    "    vec2 = cm.get_word_vector(word2).todense().flatten()\n",
    "    v1 = np.squeeze(np.asarray(vec1))\n",
    "    v2 = np.squeeze(np.asarray(vec2))\n",
    "    print('The dot product between %s and %s is %0.2f and cosine similarity is %0.2f' \n",
    "          %(word1,word2,v1.dot(v2),cosine_similarity(vec1, vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product between tall and height is 2.00 and cosine similarity is 0.71\n",
      "The dot product between tall and official is 2.00 and cosine similarity is 0.82\n"
     ]
    }
   ],
   "source": [
    "### Let's look at similarity between word pairs\n",
    "similarity('tall', 'height')\n",
    "similarity('tall', 'official')\n",
    "\n",
    "### Not very reliable similarity scores because we used only 4 sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse vs. dense word vectors\n",
    "\n",
    "- Term-term co-occurrence matrices are long and sparse. \n",
    "    - length |V|= 20,000 to 50,000\n",
    "    - most elements are zero\n",
    "- OK because there are efficient ways to deal with sparse matrices.\n",
    "\n",
    "\n",
    "### Alternative \n",
    "- Learn short (~100 to 1000 dimensions) and dense vectors. \n",
    "- Short vectors may be easier to train with ML models (less weights to train).\n",
    "- They may generalize better.\n",
    "- In practice they work much better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 2: Dense word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of algorithms to create dense word embeddings\n",
    "<img src=\"imgs/word2vec.png\" width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Activity 2: Try out Word similarity with word embeddings (~4 mins)\n",
    "\n",
    "- Go to the following online demo of word embeddings created by Turku NLP group from Finland.\n",
    "http://bionlp-www.utu.fi/wv_demo/\n",
    "- Under `Select one of the available models`, select `English GoogleNews Negative300`\n",
    "- Under `Nearest words` option, type a word and get the most similar words for the given word. Some suggestions to get you started: *UBC, bread, Computer_Science*\n",
    "- To get the similarity between two words, under `Similarity of two words`, type word pairs of your interest. Some pairs to get you started: *tall and height*, *lion and GPU*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### How can we get dense vectors?\n",
    " \n",
    "- Count-based methods\n",
    "    - Singular Value Decomposition (SVD)\n",
    "- Prediction-based methods\n",
    "    - [Word2Vec](https://github.com/tmikolov/word2vec)\n",
    "    - [fastText](https://fasttext.cc/)\n",
    "    - [GloVe](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of models to obtain dense word vectors.\n",
    "\n",
    "- Two primary algorithms \n",
    "    - **Skip-gram**\n",
    "    - Continuous bag of words (CBOW)\n",
    "- Two moderately efficient training methods \n",
    "    - Hierarchical softmax\n",
    "    - Negative sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram\n",
    "\n",
    "- A neural network model to obtain robust and dense representations of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fake word-prediction task \n",
    "\n",
    "- Given a target word (i.e., center word) word, predict context words (i.e., surrounding words). \n",
    "<blockquote>\n",
    "    Add freshly squeezed$_{context}$ pineapple$_{target}$ juice$_{context}$ to your smoothie. \n",
    "</blockquote> \n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/target_context.png\" width=\"300\" height=\"300\">\n",
    "</center>\n",
    "\n",
    "- So in the example above given the target word **pineapple**, predict whether: \n",
    "    - **juice** is likely to occur in the context of **pineapple**\n",
    "    - **squeezed** is likely to occur in the context of **pineapple** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram objective\n",
    "- Consider the conditional probabilities $p(w_c|w_t)$ and set the parameters $\\theta$ of $p(w_c|w_t; \\theta)$ so as to maximize the corpus probability. \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} p(w_c|w_t;\\theta)\n",
    "$\n",
    "</center>\n",
    "\n",
    "- $w_t$ &rarr; target word, \n",
    "- $m$ &rarr; the context window size\n",
    "- $D$ is the set of all word and context pairs from the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram objective\n",
    "\n",
    "- Model the conditional probability using softmax of the dot product.\n",
    "    * Higher the dot product higher the probability and vice-versa.     \n",
    "    \n",
    "\n",
    "$$P(w_c|w_t;\\theta) = \\frac{exp(\\vec{w_c}.\\vec{w_t})}{\\sum\\limits_{\\substack{c' \\in V}} exp(\\vec{w_{c'}}.\\vec{w_t})}\\\\\n",
    "$$\n",
    "\n",
    "- Substituting the conditional probability with the softmax of dot product: \n",
    "$$    \n",
    "\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} p(w_c|w_t;\\theta) \\approx \\prod\\limits_{(w_c,w_t) \\in D}\\frac{exp(\\vec{w_c}.\\vec{w_t})}{\\sum\\limits_{\\substack{c' \\in V}} exp(\\vec{w_{c'}}.\\vec{w_t})}$$\n",
    "- Assumption: Maximizing this objective will results in meaningful embeddings for all words in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we do it?\n",
    "\n",
    "- We use a neural network architecture with \n",
    "    - an input layer\n",
    "    - a hidden layer\n",
    "    - an output layer \n",
    "- We use the softmax activation function for the output layer. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example \n",
    "\n",
    "<img src=\"imgs/skipgram_0.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input layer and \"gold\" \n",
    "\n",
    "<img src=\"imgs/skipgram_1.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden layer\n",
    "\n",
    "<img src=\"imgs/skipgram_2.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What will be the dimensions of the weight matrix between input and hidden layers?\n",
    "\n",
    "1. $10000 \\times 1$\n",
    "2. $300 \\times 10000$\n",
    "3. $300 \\times 300$\n",
    "\n",
    "\n",
    "<img src=\"imgs/skipgram_2.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden layer and output layer \n",
    "\n",
    "<img src=\"imgs/skipgram_3.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What will be the dimensions of the weight matrix between hidden and output layers?\n",
    "\n",
    "1. $10000 \\times 1$\n",
    "2. $10000 \\times 300$\n",
    "3. $300 \\times 300$\n",
    "\n",
    "\n",
    "<img src=\"imgs/skipgram_3.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Softmax activation function \n",
    "\n",
    "- Apply softmax to get probability distribution \n",
    "\n",
    "<img src=\"imgs/skipgram_4.png\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compare prediction ($\\hat{y}$) with \"gold\" ($y$)\n",
    "\n",
    "- We want a number closer to 1 in the prediction at index 5,428\n",
    "    - Loss is high!\n",
    "\n",
    "<img src=\"imgs/skipgram_5.png\" width=\"1000\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Learn weights using backpropagation and gradient descent. \n",
    "\n",
    "<img src=\"imgs/skipgram_5.png\" width=\"500\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram model for two target-context pairs \n",
    "\n",
    "<img src=\"imgs/skip-gram.png\" width=\"1000\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Parameters to learn\n",
    "\n",
    "- Given a corpus with vocabulary of size $V$, where a word $w_i$ is identified by its index $i \\in {1, ..., V}$, learn a vector representation for each $w_i$ by predicting the words that appear in its context. \n",
    "- Learn the following parameters of the model\n",
    "    - Suppose $V = 10,000$, $d = 300$, the number of parameters to learn are 6,000,000! \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\theta = \n",
    "\\begin{bmatrix} aardvark_t\\\\\n",
    "                aback_t\\\\\n",
    "                \\dots\\\\\n",
    "                zymurgi_t\\\\\n",
    "                aardvark_c\\\\\n",
    "                aback_c\\\\                \n",
    "                \\dots\\\\\n",
    "                zymurgi_c\\\\                \n",
    "\\end{bmatrix} \\in R^{2dV}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main hyperparameters of the model\n",
    "\n",
    "- Dimensionality of the word vectors \n",
    "- Window size\n",
    "    * shorter window: more syntactic representation\n",
    "    * longer window: more semantic representation \n",
    "    * Mikolov et al. (2015) suggest setting this parameter in the range 5 to 20 for small training datasets and in the range 2 to 5 for large training datasets.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training word2vec embeddings \n",
    "\n",
    "- [Original C code](https://code.google.com/archive/p/word2vec/) \n",
    "- [GitHub version of the code](https://github.com/tmikolov/word2vec)\n",
    "- [Gensim](https://radimrehurek.com/gensim/), an open source Python library has provides a Python interface for word2vec family of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.0577651e-04, -4.1928720e-03, -4.1012844e-04, -4.5362543e-03,\n",
       "        2.8545428e-03, -2.0831355e-03, -1.5676343e-03,  2.9114417e-03,\n",
       "        4.8867948e-03, -2.0088769e-04,  4.4950220e-04, -3.1292173e-03,\n",
       "        2.3083936e-03,  2.1350672e-03,  4.1387044e-03,  4.6810219e-03,\n",
       "        3.5922842e-03, -4.8433754e-04,  7.2008377e-04, -3.0727694e-03,\n",
       "        1.6958485e-03,  1.1154150e-03,  1.2656286e-03,  4.4413866e-03,\n",
       "       -4.9861602e-04, -2.6142984e-03,  2.7393631e-03,  1.2527937e-03,\n",
       "       -4.4371560e-03, -2.0429664e-03, -2.4972567e-03, -2.0705322e-03,\n",
       "        2.5140313e-03,  4.9008396e-03,  4.0714932e-03, -1.8321427e-03,\n",
       "        3.0235352e-04, -2.1816576e-03, -3.9005610e-03,  3.6648144e-03,\n",
       "        4.5554382e-03, -4.6852673e-04, -1.5028286e-03,  2.0181837e-03,\n",
       "       -2.2745879e-04, -2.8567808e-03, -3.6195670e-03,  4.9500149e-03,\n",
       "        3.0311327e-03, -2.9000025e-03,  5.4507988e-04, -7.7085308e-04,\n",
       "        1.9078582e-03,  4.7682840e-03,  2.0714775e-03,  1.7742552e-03,\n",
       "        1.6854936e-03,  8.5244537e-04, -3.6609326e-03,  2.3385636e-03,\n",
       "       -4.7836024e-03, -4.7958856e-03,  2.7940848e-03, -3.7175504e-04,\n",
       "       -2.8047123e-04,  2.6756050e-03,  2.7235711e-03, -1.9873604e-03,\n",
       "        2.9353637e-03,  1.6168128e-03,  7.7678025e-04, -6.0118473e-04,\n",
       "        4.4109086e-03, -7.9653860e-04, -4.3881456e-03, -1.5669509e-03,\n",
       "       -2.8797910e-03,  8.1031566e-04, -7.5356796e-04,  2.4223675e-03,\n",
       "        1.7085585e-03,  1.2034380e-03,  2.8113455e-03, -2.8158179e-03,\n",
       "        2.5502136e-03, -4.7023082e-03,  3.1082956e-03, -3.6150143e-03,\n",
       "       -8.1108592e-05,  8.3733519e-04, -1.8813891e-03, -1.3152267e-05,\n",
       "       -2.3246233e-03,  4.8740436e-03, -1.7133073e-03, -4.7083325e-03,\n",
       "       -2.5045574e-03, -4.8825223e-04,  2.7066246e-03, -2.0985750e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a word2vec model on this tiny corpus\n",
    "model = Word2Vec(common_texts, \n",
    "                 size=100, \n",
    "                 window=5, \n",
    "                 min_count=1)\n",
    "\n",
    "# How does a learned dense word vector look like? \n",
    "model.wv['trees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained embeddings\n",
    "\n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "    * trained on several corpora using the word2vec algorithm \n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using [the fastText algorithm](http://aclweb.org/anthology/Q17-1010)\n",
    "    * published by Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "# You can download them from \n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  3000000\n",
      "The similarity between height and tall is 0.473\n",
      "The similarity between pineapple and mango is 0.668\n",
      "The similarity between pineapple and juice is 0.418\n",
      "The similarity between sun and robot is 0.029\n",
      "The similarity between GPU and lion is 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary: ', len(model.vocab))\n",
    "word_pairs = [('height','tall'),\n",
    "              ('pineapple','mango'), \n",
    "              ('pineapple','juice'), \n",
    "              ('sun','robot'), \n",
    "              ('GPU','lion')]\n",
    "for pair in word_pairs: \n",
    "    print('The similarity between %s and %s is %0.3f' %(pair[0], pair[1], model.similarity(pair[0], pair[1])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Success of Word2Vec\n",
    "\n",
    "- Able to capture complex relationships between words.\n",
    "- Example: What is the word that is similar to **WOMAN** in the same sense as **KING** is similar to **MAN**?\n",
    "- Perform a simple algebraic operations with the vector representation of words.\n",
    "    $\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance.\n",
    "\n",
    "<img src=\"imgs/word_analogies1.png\" width=\"500\" height=\"500\">\n",
    "(Credit: Mikolov et al. 2013)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model = model):\n",
    "    '''    \n",
    "    Returns analogy word using the given model. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    word1 -- (str) \n",
    "    word2 -- (str)\n",
    "    word3 -- (str)\n",
    "    model -- word embedding model\n",
    "    '''\n",
    "    print('%s : %s :: %s : ?' %(word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=['Analogy word', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.711819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monarch</td>\n",
       "      <td>0.618967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.590243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crown_prince</td>\n",
       "      <td>0.549946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prince</td>\n",
       "      <td>0.537732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kings</td>\n",
       "      <td>0.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Queen_Consort</td>\n",
       "      <td>0.523595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queens</td>\n",
       "      <td>0.518113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sultan</td>\n",
       "      <td>0.509859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>monarchy</td>\n",
       "      <td>0.508741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0          queen  0.711819\n",
       "1        monarch  0.618967\n",
       "2       princess  0.590243\n",
       "3   crown_prince  0.549946\n",
       "4         prince  0.537732\n",
       "5          kings  0.523684\n",
       "6  Queen_Consort  0.523595\n",
       "7         queens  0.518113\n",
       "8         sultan  0.509859\n",
       "9       monarchy  0.508741"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man','king','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montreal : Canadiens :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canucks</td>\n",
       "      <td>0.821327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vancouver_Canucks</td>\n",
       "      <td>0.750401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary_Flames</td>\n",
       "      <td>0.705470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leafs</td>\n",
       "      <td>0.695783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maple_Leafs</td>\n",
       "      <td>0.691617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thrashers</td>\n",
       "      <td>0.687504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avs</td>\n",
       "      <td>0.681716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sabres</td>\n",
       "      <td>0.665307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Blackhawks</td>\n",
       "      <td>0.664625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Habs</td>\n",
       "      <td>0.661023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Analogy word     Score\n",
       "0            Canucks  0.821327\n",
       "1  Vancouver_Canucks  0.750401\n",
       "2     Calgary_Flames  0.705470\n",
       "3              Leafs  0.695783\n",
       "4        Maple_Leafs  0.691617\n",
       "5          Thrashers  0.687504\n",
       "6                Avs  0.681716\n",
       "7             Sabres  0.665307\n",
       "8         Blackhawks  0.664625\n",
       "9               Habs  0.661023"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Montreal', 'Canadiens', 'Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Recall the title of today's lesson \n",
    "analogy('Toronto', 'UofT', 'Vancouver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implicit biases and stereotypes in word embeddings\n",
    "\n",
    "- Reflect gender stereotypes present in broader society.\n",
    "- They may also amplify these stereotypes because of their widespread usage. \n",
    "- See [this paper](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : computer_programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similar word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.562712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.510505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphic_designer</td>\n",
       "      <td>0.505180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.497949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.493489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.492551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>0.490797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saleswoman</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electrical_engineer</td>\n",
       "      <td>0.479773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanical_engineer</td>\n",
       "      <td>0.475540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Similar word     Score\n",
       "0            homemaker  0.562712\n",
       "1            housewife  0.510505\n",
       "2     graphic_designer  0.505180\n",
       "3        schoolteacher  0.497949\n",
       "4        businesswoman  0.493489\n",
       "5            paralegal  0.492551\n",
       "6     registered_nurse  0.490797\n",
       "7           saleswoman  0.488163\n",
       "8  electrical_engineer  0.479773\n",
       "9  mechanical_engineer  0.475540"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'computer_programmer', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity: Explore analogies with word embeddings (~4 mins)\n",
    "\n",
    "- Again go to following online demo. \n",
    "http://bionlp-www.utu.fi/wv_demo/ \n",
    "- Team up with your neighbour and try out some pairs for analogies together (~3 minutes).\n",
    "- Class discussion (~2 minutes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Vector space model \n",
    "    * Modeling word meaning by placing it in a vector space.\n",
    "    * Distance between words in this vector space indicate the relationship between them. \n",
    "- Word embeddings\n",
    "    * Sparse embeddings using co-occurrence matrix\n",
    "    * Dense embeddings using word2vec models \n",
    "        * Freely available code and pre-trained models \n",
    "        * Available for many different languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Revisit: Learning outcomes\n",
    "\n",
    "\n",
    "1. Word representation created by term-term co-occurrence matrix are long and sparse whereas the ones created by word2vec models are short and dense. True or False? \n",
    "3. Given the following table, which word pair is more similar in terms of dot product: the word pair in A or in B?  \n",
    "    1. X, Y\n",
    "    2. X, Z\n",
    "\n",
    "<img src=\"imgs/similarity_question.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "2. The skip-gram model predicts context word given a target word. True or False? \n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up \n",
    "\n",
    "- Wait! Don't we want to represent sentences and paragraphs for tasks such as below? \n",
    "          \n",
    "$X = \\begin{bmatrix}\\text{\"@united you're terrible. You don't understand safety\",}\\\\ \\text{\"@JetBlue safety first !! #lovejetblue\"}\\\\ \\text{\"@SouthwestAir truly the best in #customerservice!\"}\\\\ \\end{bmatrix}$ and $y = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "- We will build on the idea of word embeddings to come up with general text representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonus: Relevant papers\n",
    "\n",
    "- [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [Efficient estimation of word representations in vector space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Linguistic regularities in continuous space word representations](https://www.aclweb.org/anthology/N13-1090)\n",
    "- [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Bonus: Examples of semantic and syntactic relationships\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/word_analogies2.png\" width=\"800\" height=\"800\">\n",
    "(Credit: Mikolov 2013)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonus: Links for pre-trained embeddings\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "\n",
    "### Bonus: Fun tools\n",
    "[wevi: word embedding visual inspector](https://ronxin.github.io/wevi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reflection (~10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reflection: Activities \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### My vision for MDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### My vision for CS\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vision \n",
    "\n",
    "- TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
