{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UBC instructor position sample lesson  \n",
    "\n",
    "### By Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "### Today's plan\n",
    "\n",
    "- Set the stage (~5 mins)\n",
    "- Sample class (~40 mins)\n",
    "- Reflection (~10 mins)\n",
    "- Vision (~10 mins)\n",
    "- Q and A (~10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Set the stage (~5 mins)\n",
    "\n",
    "- I am envisioning this as the second lesson of DSCI 575 (Advanced Machine Learning in the context of Natural Language Processing (NLP) applications). \n",
    "- It is the first week of the final block of the MDS program's curriculum. \n",
    "- The students have already taken four Machine Learning courses: DSCI 571, DSCI 572, DSCI 573, DSCI 563. \n",
    "- They are now ready to apply the concepts they have learned so far on interesting problems. \n",
    "- The prerequisites I am assuming are\n",
    "    - Familiarity with neural networks, which they have done in DSCI 572."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSCI 575: Advanced Machine Learning (in the context of NLP applications)\n",
    "\n",
    "## Lecture 2: Introduction to Word Embeddings: _Toronto_ : _UofT_ :: _Vancouver_ : ?\n",
    "Instructor: Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "\n",
    "#### Today's plan\n",
    "- Recap from lecture 1 ()\n",
    "- Activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "common_texts\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "from preprocessing import MyPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN STARTER CODE\n",
    "class CooccurrenceMatrix:\n",
    "    def __init__(self, corpus, \n",
    "                       tokenizer = word_tokenize, \n",
    "                       window_size = 3):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.window_size = window_size\n",
    "        self.vocab = {}\n",
    "        self.cooccurrence_matrix = None    \n",
    "        \n",
    "    def fit_transform(self):\n",
    "        \"\"\"\n",
    "        Creates a co-occurrence matrix. \n",
    "        \n",
    "        Returns vocabulary (dict) and co-occurrence matrix (csr_matrix)\n",
    "        \"\"\"\n",
    "        data=[]\n",
    "        row=[]\n",
    "        col=[]\n",
    "        for tokens in self.corpus:\n",
    "            for target_index, token in enumerate(tokens):\n",
    "                # Get the index of the word in the vocabulary. If the word is not in the vocabulary, \n",
    "                # set the index to the size of the vocabulary. \n",
    "                i = self.vocab.setdefault(token, len(self.vocab))\n",
    "                \n",
    "                # Consider the context words depending upon the context window \n",
    "                start = max(0, target_index - self.window_size)\n",
    "                end = min(len(tokens), target_index + self.window_size + 1)\n",
    "                \n",
    "                for context_index in range(start, end):\n",
    "                    # Do not consider the target word.  \n",
    "                    if target_index == context_index: \n",
    "                        continue                        \n",
    "                    j = self.vocab.setdefault(tokens[context_index], len(self.vocab))\n",
    "                    # Set diagonal to 0\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    data.append(1.0); row.append(i); col.append(j);\n",
    "        self.cooccurrence_matrix = csr_matrix((data,(row,col)))\n",
    "        return self.vocab, self.cooccurrence_matrix\n",
    "            \n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"\n",
    "        Given a word returns the word vector associated with it from the co-occurrence matrix. \n",
    "\n",
    "        Keyword arguments:\n",
    "        word -- (str) the word to look up in the vocab.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # BEGIN SOLUTION\n",
    "        if word in self.vocab: \n",
    "            return self.cooccurrence_matrix[self.vocab[word]]\n",
    "        else:\n",
    "            print('The word not present in the vocab')\n",
    "        # END SOLUTION\n",
    "\n",
    "# END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Last class: What is Natural Language Processing (NLP)?\n",
    "#### How often do you search everyday? \n",
    "<img src=\"images/Google_search.png\" width=\"900\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last class: What is Natural Language Processing (NLP)?\n",
    "\n",
    "<img src=\"images/WhatisNLP.png\" width=\"900\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is it hard?\n",
    "\n",
    "- Language is complex and subtle. \n",
    "- All the problems related to representation and reasoning in artificial intelligence arise in this domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Lexical ambiguity\n",
    "\n",
    "- Language is ambiguous at different levels. \n",
    "\n",
    "<img src=\"images/lexical_ambiguity.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Referential ambiguity\n",
    "\n",
    " - Language understanding involves common-sense knowledge and real-world reasoning.\n",
    " \n",
    "<img src=\"files/images/referential_ambiguity.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representing text \n",
    "\n",
    "- So far we have been using data that looks like this: \n",
    "\n",
    "$X = \\begin{bmatrix}1 & 0.8 & 0.3\\\\ 0 & 0 & 0.4\\\\ 1 & 0.2 & 0.8\\\\ \\end{bmatrix}$ and $y = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "- But consider data that looks like this instead: \n",
    "\n",
    "          \n",
    "$X = \\begin{bmatrix}\\text{\"@united you're terrible. You don't understand safety\",}\\\\ \\text{\"@JetBlue safety first !! #lovejetblue\"}\\\\ \\text{\"@SouthwestAir truly the best in #customerservice!\"}\\\\ \\end{bmatrix}$ and $y = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "- ML algorithms we have seen so far prefer well-defined and fixed length input/output and text data is usually messy. \n",
    "- How can we effectively represent these reviews using features? \n",
    "    - Ideally we want to capture the \"meaning\" of text in our representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Today's promise\n",
    "\n",
    "- A method that learns a powerful representation of text data.  \n",
    "\n",
    "#### Learning outcomes\n",
    "\n",
    "From this class, you will be able to \n",
    "\n",
    "- Explain the general idea of the vector space models and co-occurrence matrices.\n",
    "- Explain the difference between sparse and dense word vectors.\n",
    "- Explain the general idea of the continuous skip-gram model.\n",
    "- Train your own word vectors with Gensim and work with dense word vectors. \n",
    "- Load pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning: _cup_ or _bowl_?\n",
    "\n",
    "- Before moving to meaning of a sentence or a paragraph, let's start with word meaning. \n",
    "- Philosophical debate for centuries \n",
    "- Example: Where does the category cup end?\n",
    "- Labov, 1975\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/Labov_cups.png\" width=\"600\" height=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### [Are hockey gloves gloves or \"articles of plastics\"?](https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258)\n",
    "\n",
    "<blockquote>\n",
    "Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either gloves or as \"articles of plastics.\"\n",
    "</blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning: NLP view\n",
    "- Modeling word meaning that allows us to \n",
    "    * draw useful inferences to solve meaning-related problems \n",
    "    * find relationship between words, e.g., which words are similar, which ones have positive or negative connotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pair-share (~5 mins) \n",
    "\n",
    "- Suppose you are building a Question Answering system and you are given the following question and three candidate answers. \n",
    "- Discuss the following questions with your neighbour(s). \n",
    "    - What kind of relationship between words do we need to capture in order to arrive at the correct answer?  \n",
    "    - Brainstorm ways to represent words that will allow you to capture this relationship.   \n",
    "<blockquote>       \n",
    "<p style=\"font-size:30px\"><b>Question:</b> How <b>tall</b> is Machu Picchu?</p>\n",
    "    <p style=\"font-size:30px\"><b>Candidate 1:</b> Machu Picchu is 13.164 degrees south of the equator.</p>    \n",
    "<p style=\"font-size:30px\"><b>Candidate 2:</b> The official height of Machu Picchu is 2,430 m.</p>\n",
    "<p style=\"font-size:30px\"><b>Candidate 3:</b> Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.</p>    \n",
    "</blockquote> \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tall', 'machu', 'picchu'],\n",
       " ['machu', 'picchu', '13.164', 'degrees', 'south', 'equator'],\n",
       " ['official', 'height', 'machu', 'picchu', '2,430'],\n",
       " ['machu', 'picchu', '80', 'kilometres', '50', 'miles', 'northwest', 'cusco']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"How tall is Machu Picchu?\",\n",
    "          \"Machu Picchu is 13.164 degrees south of the equator.\", \n",
    "          \"The official height of Machu Picchu is 2,430 m.\",\n",
    "          \"Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.\"\n",
    "         ]\n",
    "pp = MyPreprocessor()\n",
    "pp_corpus = pp.preprocess_corpus(corpus)\n",
    "pp_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer = nltk.word_tokenize)\n",
    "X = vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>.</th>\n",
       "      <th>13.164</th>\n",
       "      <th>2,430</th>\n",
       "      <th>50</th>\n",
       "      <th>80</th>\n",
       "      <th>?</th>\n",
       "      <th>cusco</th>\n",
       "      <th>degrees</th>\n",
       "      <th>...</th>\n",
       "      <th>m</th>\n",
       "      <th>machu</th>\n",
       "      <th>miles</th>\n",
       "      <th>northwest</th>\n",
       "      <th>of</th>\n",
       "      <th>official</th>\n",
       "      <th>picchu</th>\n",
       "      <th>south</th>\n",
       "      <th>tall</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>How tall is Machu Picchu?</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machu Picchu is 13.164 degrees south of the equator.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The official height of Machu Picchu is 2,430 m.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    (  )  .  13.164  2,430  \\\n",
       "How tall is Machu Picchu?                           0  0  0       0      0   \n",
       "Machu Picchu is 13.164 degrees south of the equ...  0  0  1       1      0   \n",
       "The official height of Machu Picchu is 2,430 m.     0  0  1       0      1   \n",
       "Machu Picchu is 80 kilometres (50 miles) northw...  1  1  1       0      0   \n",
       "\n",
       "                                                    50  80  ?  cusco  degrees  \\\n",
       "How tall is Machu Picchu?                            0   0  1      0        0   \n",
       "Machu Picchu is 13.164 degrees south of the equ...   0   0  0      0        1   \n",
       "The official height of Machu Picchu is 2,430 m.      0   0  0      0        0   \n",
       "Machu Picchu is 80 kilometres (50 miles) northw...   1   1  0      1        0   \n",
       "\n",
       "                                                    ...  m  machu  miles  \\\n",
       "How tall is Machu Picchu?                           ...  0      1      0   \n",
       "Machu Picchu is 13.164 degrees south of the equ...  ...  0      1      0   \n",
       "The official height of Machu Picchu is 2,430 m.     ...  1      1      0   \n",
       "Machu Picchu is 80 kilometres (50 miles) northw...  ...  0      1      1   \n",
       "\n",
       "                                                    northwest  of  official  \\\n",
       "How tall is Machu Picchu?                                   0   0         0   \n",
       "Machu Picchu is 13.164 degrees south of the equ...          0   1         0   \n",
       "The official height of Machu Picchu is 2,430 m.             0   1         1   \n",
       "Machu Picchu is 80 kilometres (50 miles) northw...          1   1         0   \n",
       "\n",
       "                                                    picchu  south  tall  the  \n",
       "How tall is Machu Picchu?                                1      0     1    0  \n",
       "Machu Picchu is 13.164 degrees south of the equ...       1      1     0    1  \n",
       "The official height of Machu Picchu is 2,430 m.          1      0     0    1  \n",
       "Machu Picchu is 80 kilometres (50 miles) northw...       1      0     0    0  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check whether bag of words representation is better\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=sorted(vec.vocabulary_), index=corpus)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How tall is Machu Picchu? \n",
      "Candidate: Machu Picchu is 13.164 degrees south of the equator. \n",
      "Similarity = 3.000\n",
      "\n",
      "\n",
      "Question: How tall is Machu Picchu? \n",
      "Candidate: The official height of Machu Picchu is 2,430 m. \n",
      "Similarity = 3.000\n",
      "\n",
      "\n",
      "Question: How tall is Machu Picchu? \n",
      "Candidate: Machu Picchu is 80 kilometres (50 miles) northwest of Cusco. \n",
      "Similarity = 3.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Let's examine the similarity between these three documents\n",
    "for i in range(1,4,1):\n",
    "    print('Question: %s \\nCandidate: %s \\nSimilarity = %0.3f\\n\\n' \\\n",
    "                          %(bow_df.index[0],bow_df.index[i], \\\n",
    "                            bow_df.iloc[0].dot(bow_df.iloc[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 1: Representing words as atomic symbols\n",
    "\n",
    "- Build the **vocabulary** containing all unique words from the corpus. \n",
    "- Represent each word as **one-hot** encoding. \n",
    "- A vector of length $V$ such that the value at word index is 1 and all other indices is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_onehot_encoding(word, vocab):\n",
    "    onehot = np.zeros(len(vocab), dtype='float64')    \n",
    "    onehot[vocab[word]] = 1\n",
    "    print('one-hot encoding of the word \"%s\" is: %s' % (word, str(onehot)))\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding of the word \"tall\" is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0.]\n",
      "one-hot encoding of the word \"height\" is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "The dot product between tall and height is: 0\n"
     ]
    }
   ],
   "source": [
    "# Note: In the NLP community a text data set is referred \n",
    "# to as a **corpus** (plural: corpora).\n",
    "\n",
    "vocab = vec.vocabulary_\n",
    "word1 = 'tall'\n",
    "onehot_word1 = get_onehot_encoding(word1, vocab)\n",
    "\n",
    "word2 = 'height'\n",
    "onehot_word2 = get_onehot_encoding(word2, vocab)\n",
    "\n",
    "print(\"The dot product between %s and %s is: %d\" % \n",
    "      (word1, word2, onehot_word1.dot(onehot_word2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem with one-hot encoding\n",
    "\n",
    "-  The problem with this representation is that there is no inherent notion of relationship between words.\n",
    "\n",
    "<center>\n",
    "$\\vec{height}.\\vec{tall} = 0$ \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 2: Term-term co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributional hypothesis\n",
    "\n",
    "<blockquote> \n",
    "    <p>You shall know a word by the company it keeps.</p>\n",
    "    <footer>Firth, 1957</footer>        \n",
    "</blockquote>\n",
    "\n",
    "<blockquote> \n",
    "If A and B have almost identical environments we say that they are synonyms.\n",
    "<footer>Harris, 1954</footer>    \n",
    "</blockquote>    \n",
    "\n",
    "Example: \n",
    "\n",
    "- Her **child** loves to play in the playground. \n",
    "- Her **kid** loves to play in the playground. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector space model\n",
    "\n",
    "- A standard way to represent meaning in NLP\n",
    "- Model the meaning of a word by placing it into a vector space.  \n",
    "- Distances among words in the vector space indicate the relationship between them. \n",
    "- Called an \"embedding\" because it's embedded into a high-dimensional space\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/t-SNE_word_embeddings.png\" width=\"700\" height=\"700\">\n",
    "    (Attribution: Jurafsky and Martin 3rd edition)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing word vectors and similarity \n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/word_vectors_and_angles.png\" width=\"600\" height=\"600\">\n",
    "(Credit: Jurafsky and Martin 3rd edition)\n",
    "</center>\n",
    "\n",
    "- $similarity_{cosine}(\\vec{w_1},\\vec{w_2}) = \\frac{\\vec{w_1}.\\vec{w_2}}{\\left\\lVert w_1\\right\\rVert_2 \\left\\lVert w_2\\right\\rVert_2}$ \n",
    "\n",
    "- $similarity_{cosine}(\\vec{\\text{digital}},\\vec{\\text{information}}) = \\frac{0 \\times 1 + 1 \\times 6}{\\sqrt{1} \\sqrt{1 + 36}} = 0.98$\n",
    "\n",
    "- $similarity_{cosine}(\\vec{\\text{apricot}},\\vec{\\text{information}}) = \\frac{2 \\times 1 + 0 \\times 6}{\\sqrt{4}\\sqrt{1 + 36}} = 0.16$\n",
    "\n",
    "- Often just the dot product is also used as a measure of similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tall</th>\n",
       "      <th>machu</th>\n",
       "      <th>picchu</th>\n",
       "      <th>13.164</th>\n",
       "      <th>degrees</th>\n",
       "      <th>south</th>\n",
       "      <th>equator</th>\n",
       "      <th>official</th>\n",
       "      <th>height</th>\n",
       "      <th>2,430</th>\n",
       "      <th>80</th>\n",
       "      <th>kilometres</th>\n",
       "      <th>50</th>\n",
       "      <th>miles</th>\n",
       "      <th>northwest</th>\n",
       "      <th>cusco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tall</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machu</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picchu</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.164</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tall  machu  picchu  13.164  degrees  south  equator  official  \\\n",
       "tall        0      1       1       0        0      0        0         0   \n",
       "machu       1      0       4       1        1      0        0         1   \n",
       "picchu      1      4       0       1        1      1        0         1   \n",
       "13.164      0      1       1       0        1      1        1         0   \n",
       "degrees     0      1       1       1        0      1        1         0   \n",
       "\n",
       "         height  2,430  80  kilometres  50  miles  northwest  cusco  \n",
       "tall          0      0   0           0   0      0          0      0  \n",
       "machu         1      1   1           1   0      0          0      0  \n",
       "picchu        1      1   1           1   1      0          0      0  \n",
       "13.164        0      0   0           0   0      0          0      0  \n",
       "degrees       0      0   0           0   0      0          0      0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CooccurrenceMatrix(pp_corpus)\n",
    "vocab, comat = cm.fit_transform()\n",
    "words = [key for key, value in sorted(vocab.items(), key = lambda item: (item[1],item[0]))]\n",
    "df = pd.DataFrame(comat.todense(), \n",
    "                  columns = words, \n",
    "                  index = words,\n",
    "                  dtype = np.int8\n",
    "                 )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "vec1 = cm.get_word_vector('tall')\n",
    "vec2 = cm.get_word_vector('height')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse vs. dense word vectors\n",
    "\n",
    "- Term-term co-occurrence matrices are long and sparse. \n",
    "    - length |V|= 20,000 to 50,000\n",
    "    - most elements are zero\n",
    "- OK because there are efficient ways to deal with sparse matrices.\n",
    "\n",
    "\n",
    "### Alternative \n",
    "- Learn short (~100 to 1000 dimensions) and dense vectors. \n",
    "- Short vectors may be easier to train with ML models (less weights to train).\n",
    "- They may generalize better.\n",
    "- In practice they work much better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 3: Dense word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of algorithms to create dense word embeddings\n",
    "- $|V| \\rightarrow$ vocabulary size\n",
    "- $d \\rightarrow$ number of dimensions \n",
    "<img src=\"images/word2vec.png\" width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pair-share (~5 mins)\n",
    "\n",
    "- Before we go into the details of the algorithm, let's examine what can we do with word embeddings. \n",
    "- Let's try an online demo of word embeddings created by Turku NLP group from Finland. \n",
    "- Explore similarities between words pairs of your choice. \n",
    "\n",
    "http://bionlp-www.utu.fi/wv_demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Success of Word2Vec\n",
    "\n",
    "- Able to capture complex relationships between words.\n",
    "- Example: What is the word that is similar to **WOMAN** in the same sense as **KING** is similar to **MAN**?\n",
    "- Perform a simple algebraic operations with the vector representation of words.\n",
    "    $\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance.\n",
    "\n",
    "<img src=\"files/images/word_analogies1.png\" width=\"500\" height=\"500\">\n",
    "(Credit: Mikolov et al. 2013)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we get dense vectors?\n",
    " \n",
    "- Count-based methods\n",
    "    - Singular Value Decomposition (SVD)\n",
    "- Prediction-based methods\n",
    "    - [Word2Vec](https://github.com/tmikolov/word2vec)\n",
    "    - [fastText](https://fasttext.cc/)\n",
    "    - [GloVe](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of models to get word vectors.\n",
    "\n",
    "- Two primary algorithms \n",
    "    - **Skip-gram**\n",
    "    - Continuous bag of words\n",
    "\n",
    "- Two moderately efficient training methods \n",
    "    - Hierarchical softmax\n",
    "    - negative sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fake word-prediction task \n",
    "\n",
    "- Given a target word, predict context words. \n",
    "<center>\n",
    "<blockquote>\n",
    "    Add freshly squeezed <b>pineapple</b> juice to your smoothie. \n",
    "</blockquote> \n",
    "</center>\n",
    "\n",
    "- Target word: **pineapple**\n",
    "- Is the word **juice** likely to occur in the context of **pineapple**? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context words and target word\n",
    "\n",
    "- Given a *target* word, we define a **context window** and predict each word in the given context window.\n",
    "- If the context window is $n$, consider $n$ preceding and $n$ following words of the target word.\n",
    "- Example \n",
    "    * Sequence of words &rarr;  $w_{t-3}, w_{t-2}, w_{t-1}, w_t, w_{t+1}, w_{t+2}$\n",
    "    * Target word &rarr; $w_t$\n",
    "    * $n=2$\n",
    "    * Context words &rarr;  $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$.\n",
    "- Context window does not usually cross sentence boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context words and target word\n",
    "\n",
    "<center>\n",
    "<blockquote>\n",
    "    Add freshly squeezed <b>pineapple</b> juice to your smoothie . It will make your smoothie extra special . \n",
    "</blockquote> \n",
    "</center>\n",
    "\n",
    "- What are the context words of the target word **pineapple** with the window size 2? \n",
    "- What are the context words of the target word **smoothie** with the window size 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General idea\n",
    "\n",
    "- We want to predict a single word and words in its context similar to what we saw in distributional representations.\n",
    "- $P(context|word)$\n",
    "- Once we have that we define a loss function \n",
    "\n",
    "    $$Loss = 1 - P(context|word)$$ \n",
    "- If we predict them perfectly, the $P(context|word)$ is 1, we have no loss\n",
    "    \n",
    "    \n",
    "- We do that for many word and context pairs in a large corpus\n",
    "- Keep adjusting the weights to minimize the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous skip-gram objective\n",
    "- Consider the conditional probabilities $p(w_c|w_t)$ and set the parameters $\\theta$ of $p(w_c|w_t; \\theta)$ so as to maximize the corpus probability. \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\arg\\max\\limits_{\\substack{\\theta}} \\prod\\limits_{\\substack{t=1}}^{\\substack{T}}\\prod\\limits_{\\substack{-m \\leq c \\leq +m \\\\ c\\neq0}} p(w_c|w_t;\\theta)\\\\\n",
    "\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} p(w_c|w_t;\\theta)\n",
    "$\n",
    "</center>\n",
    "\n",
    "- $w_t$ &rarr; target word, \n",
    "- $m$ &rarr; the context window size\n",
    "- $D$ is the set of all word and context pairs from the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters to learn\n",
    "\n",
    "- Given a corpus with vocabulary of size $V$, where a word $w_i$ is identified by its index $i \\in {1, ..., V}$, learn a vector representation for each $w_i$ by predicting the words that appear in its context. \n",
    "- Learn the following parameters of the model\n",
    "    - Suppose $V = 10,000$, $d = 300$, the number of parameters to learn are 6,000,000! \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\theta = \n",
    "\\begin{bmatrix} aardvark_t\\\\\n",
    "                aback_t\\\\\n",
    "                \\dots\\\\\n",
    "                zymurgi_t\\\\\n",
    "                aardvark_c\\\\\n",
    "                aback_c\\\\                \n",
    "                \\dots\\\\\n",
    "                zymurgi_c\\\\                \n",
    "\\end{bmatrix} \\in R^{2dV}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling conditional probability\n",
    "\n",
    "- Model the conditional probability using softmax of the dot product.\n",
    "    * Higher the dot product higher the probability and vice-versa.     \n",
    "    \n",
    "<center>\n",
    "$P(w_c|w_t;\\theta) = \\frac{exp(\\vec{w_c}.\\vec{w_t})}{\\sum\\limits_{\\substack{c' \\in V}} exp(\\vec{w_{c'}}.\\vec{w_t})}\\\\\n",
    "$\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous skip-gram objective\n",
    "   \n",
    "<center>\n",
    "$    \n",
    "\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} p(w_c|w_t;\\theta) \\approx \\prod\\limits_{(w_c,w_t) \\in D}\\frac{exp(\\vec{w_c}.\\vec{w_t})}{\\sum\\limits_{\\substack{c' \\in V}} exp(\\vec{w_{c'}}.\\vec{w_t})}\\\\\n",
    "\\arg \\max\\limits_\\theta \\sum\\limits_{(w_c,w_t) \\in D} log\\mbox{ }p(w_c|w_t;\\theta) \\approx \\sum\\limits_{(w_c,w_t) \\in D} (log\\mbox{ }exp(\\vec{w_c}.\\vec{w_t})  - log \\sum\\limits_{\\substack{c' \\in V}} exp(\\vec{w_{c'}}.\\vec{w_t}))\n",
    "$    \n",
    "</center>    \n",
    "\n",
    "\n",
    "- Assumption: Maximizing this objective will results in meaningful embeddings for all words in the vocabulary. \n",
    "- Learn two embeddings for each word: context embedding and target embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/skip-gram.png\" width=\"1000\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main hyperparameters of the model\n",
    "\n",
    "- Dimensionality of the word vectors \n",
    "- Window size\n",
    "    * shorter window: more syntactic representation\n",
    "    * longer window: more semantic representation \n",
    "    * Mikolov et al. (2015) suggest setting this parameter in the range 5 to 20 for small training datasets and in the range 2 to 5 for large training datasets.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training word2vec embeddings \n",
    "\n",
    "- [Original C code](https://code.google.com/archive/p/word2vec/) \n",
    "- [GitHub version of the code](https://github.com/tmikolov/word2vec)\n",
    "- [Gensim](https://radimrehurek.com/gensim/), an open source Python library has provides a Python interface for word2vec family of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00055038, -0.00025228,  0.00476185, -0.00422345, -0.00320533,\n",
       "        0.00493144,  0.00104723,  0.00467511, -0.00425784,  0.00324989,\n",
       "       -0.00267231,  0.00352778,  0.00219797, -0.00090192,  0.0031217 ,\n",
       "        0.00375813, -0.0028545 , -0.00256043,  0.00247986,  0.00249211,\n",
       "        0.00067601,  0.00359497, -0.0019647 ,  0.00037799,  0.00037788,\n",
       "       -0.0026525 ,  0.00303038,  0.00310917,  0.00252059,  0.00350445,\n",
       "       -0.00371354,  0.00395391, -0.00496338, -0.0031164 ,  0.00135512,\n",
       "        0.00201187, -0.00470974, -0.00464884,  0.00060289, -0.00069758,\n",
       "        0.00229744, -0.00397985,  0.00255192, -0.00015335, -0.00186081,\n",
       "       -0.00260885, -0.00122199,  0.00491633,  0.00323905, -0.00066195,\n",
       "       -0.00134309,  0.00428094,  0.00287925,  0.00470387,  0.00444078,\n",
       "       -0.00140785,  0.0042861 ,  0.00238582,  0.00021381, -0.00388645,\n",
       "        0.00193228, -0.00160409,  0.00019206, -0.00120234,  0.00479675,\n",
       "       -0.00296858,  0.00301671,  0.00423891,  0.00194561,  0.00101003,\n",
       "        0.00242563,  0.00253131, -0.00121365,  0.00026648, -0.00426763,\n",
       "        0.00325865, -0.00183097,  0.00493728, -0.00416182,  0.00389653,\n",
       "        0.00156408,  0.00476936,  0.00035125, -0.00398966, -0.00416502,\n",
       "        0.00332173,  0.00121836,  0.00187991,  0.00465643, -0.00299049,\n",
       "        0.00190258, -0.00496431,  0.00255346, -0.0027609 , -0.00263793,\n",
       "        0.00387022,  0.00301274, -0.00420582,  0.00290776,  0.00388455],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"How tall is Machu Picchu?\",\n",
    "          \"Machu Picchu is 13.164 degrees south of the equator.\", \n",
    "          \"The official height of Machu Picchu is 2,430 m.\",\n",
    "          \"Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.\"\n",
    "         ]\n",
    "pp = MyPreprocessor()\n",
    "pp_corpus = pp.preprocess_corpus(corpus)\n",
    "pp_corpus\n",
    "\n",
    "model = Word2Vec(pp_corpus, \n",
    "                 size=100, \n",
    "                 window=5, \n",
    "                 min_count=1,\n",
    "                 sg=1, \n",
    "                 negative=4, \n",
    "                )\n",
    "model.wv['tall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other popular methods to get embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained embeddings\n",
    "\n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "    * trained on several corpora using the word2vec algorithm \n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using [the fastText algorithm](http://aclweb.org/anthology/Q17-1010)\n",
    "    * published by Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae497505d914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Size of vocabulary: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m word_pairs = [('height','tall'),\n\u001b[1;32m      3\u001b[0m               \u001b[0;34m(\u001b[0m\u001b[0;34m'pineapple'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mango'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0;34m(\u001b[0m\u001b[0;34m'pineapple'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'juice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               ('sun','robot')]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary: ', len(model.wv.vocab))\n",
    "word_pairs = [('height','tall'),\n",
    "              ('pineapple','mango'), \n",
    "              ('pineapple','juice'), \n",
    "              ('sun','robot')]\n",
    "for pair in word_pairs: \n",
    "    print('The similarity between %s and %s is %0.3f' %(pair[0], pair[1], model.similarity(pair[0], pair[1])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained embeddings\n",
    "\n",
    "Why? \n",
    "- Training embeddings is computationally expensive\n",
    "- For large corpora, the vocabulary size is more that 100,000.  \n",
    "- If the size of embeddings is 300, the number of parameters of the model is $2 \\times 30,000,000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relational meaning between words\n",
    "\n",
    "- Complex similarities between words\n",
    "    - **big** : **biggest** :: **small** : ?\n",
    "- Perform a simple algebraic operations with the vector representation of words\n",
    "    $\\vec{X} = \\vec{biggest} − \\vec{big} + \\vec{small}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/word_analogies1.png\" width=\"500\" height=\"500\">\n",
    "(Credit: Mikolov et al. 2013)    \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Examples of semantic and syntactic relationships\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/word_analogies2.png\" width=\"800\" height=\"800\">\n",
    "(Credit: Mikolov 2013)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model = w2v_model):\n",
    "    '''    \n",
    "    Returns analogy word using the given model. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    word1 -- (str) \n",
    "    word2 -- (str)\n",
    "    word3 -- (str)\n",
    "    model -- word embedding model\n",
    "    '''\n",
    "    print('%s : %s :: %s : ?' %(word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=['Analogy word', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.711819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monarch</td>\n",
       "      <td>0.618967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.590243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crown_prince</td>\n",
       "      <td>0.549946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prince</td>\n",
       "      <td>0.537732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kings</td>\n",
       "      <td>0.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Queen_Consort</td>\n",
       "      <td>0.523595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queens</td>\n",
       "      <td>0.518113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sultan</td>\n",
       "      <td>0.509859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>monarchy</td>\n",
       "      <td>0.508741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0          queen  0.711819\n",
       "1        monarch  0.618967\n",
       "2       princess  0.590243\n",
       "3   crown_prince  0.549946\n",
       "4         prince  0.537732\n",
       "5          kings  0.523684\n",
       "6  Queen_Consort  0.523595\n",
       "7         queens  0.518113\n",
       "8         sultan  0.509859\n",
       "9       monarchy  0.508741"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man','king','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montreal : Canadiens :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canucks</td>\n",
       "      <td>0.821327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vancouver_Canucks</td>\n",
       "      <td>0.750401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary_Flames</td>\n",
       "      <td>0.705470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leafs</td>\n",
       "      <td>0.695783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maple_Leafs</td>\n",
       "      <td>0.691617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thrashers</td>\n",
       "      <td>0.687504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avs</td>\n",
       "      <td>0.681716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sabres</td>\n",
       "      <td>0.665307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Blackhawks</td>\n",
       "      <td>0.664625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Habs</td>\n",
       "      <td>0.661023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Analogy word     Score\n",
       "0            Canucks  0.821327\n",
       "1  Vancouver_Canucks  0.750401\n",
       "2     Calgary_Flames  0.705470\n",
       "3              Leafs  0.695783\n",
       "4        Maple_Leafs  0.691617\n",
       "5          Thrashers  0.687504\n",
       "6                Avs  0.681716\n",
       "7             Sabres  0.665307\n",
       "8         Blackhawks  0.664625\n",
       "9               Habs  0.661023"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Montreal', 'Canadiens', 'Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft : Windows :: Apple : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Macs</td>\n",
       "      <td>0.673568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iMac</td>\n",
       "      <td>0.646340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac_OS</td>\n",
       "      <td>0.640714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>0.640588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPad</td>\n",
       "      <td>0.633464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OS_X</td>\n",
       "      <td>0.632136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iBook</td>\n",
       "      <td>0.626197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iMacs</td>\n",
       "      <td>0.619245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iOS</td>\n",
       "      <td>0.617178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mac_mini</td>\n",
       "      <td>0.611140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0         Macs  0.673568\n",
       "1         iMac  0.646340\n",
       "2       Mac_OS  0.640714\n",
       "3       iPhone  0.640588\n",
       "4         iPad  0.633464\n",
       "5         OS_X  0.632136\n",
       "6        iBook  0.626197\n",
       "7        iMacs  0.619245\n",
       "8          iOS  0.617178\n",
       "9     Mac_mini  0.611140"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Microsoft', 'Windows', 'Apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implicit biases and stereotypes in word embeddings\n",
    "\n",
    "- Reflect gender stereotypes present in broader society.\n",
    "- They may also amplify these stereotypes because of their widespread usage. \n",
    "- See [this paper](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : computer_programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similar word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.562712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.510505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphic_designer</td>\n",
       "      <td>0.505180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.497949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.493489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.492551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>0.490797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saleswoman</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electrical_engineer</td>\n",
       "      <td>0.479773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanical_engineer</td>\n",
       "      <td>0.475540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Similar word     Score\n",
       "0            homemaker  0.562712\n",
       "1            housewife  0.510505\n",
       "2     graphic_designer  0.505180\n",
       "3        schoolteacher  0.497949\n",
       "4        businesswoman  0.493489\n",
       "5            paralegal  0.492551\n",
       "6     registered_nurse  0.490797\n",
       "7           saleswoman  0.488163\n",
       "8  electrical_engineer  0.479773\n",
       "9  mechanical_engineer  0.475540"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'computer_programmer', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pair-share (5 mins)\n",
    "\n",
    "- Again go to following online demo. \n",
    "http://bionlp-www.utu.fi/wv_demo/ \n",
    "- Team up with your neighbour and try out some pairs for analogies together (~3 minutes).\n",
    "- Class discussion (~2 minutes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Vector space model \n",
    "    * Modeling word meaning by placing it in a vector space.\n",
    "    * Distance between words in this vector space indicate the relationship between them. \n",
    "- Word embeddings\n",
    "    * Creating short and dense representations of words. \n",
    "- Word2Vec\n",
    "    * A family of models to learn dense vector representations of words\n",
    "    * Freely available code and pre-trained models \n",
    "    * Available for many different languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant papers\n",
    "\n",
    "- [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [Efficient estimation of word representations in vector space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Linguistic regularities in continuous space word representations](https://www.aclweb.org/anthology/N13-1090)\n",
    "- [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Links for pre-trained embeddings\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "\n",
    "## Fun tools\n",
    "[wevi: word embedding visual inspector](https://ronxin.github.io/wevi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reflection \n",
    "+  10 minute  reflection:  Once you  complete the  sample  class, we  stop\n",
    "  role-playing and return to behaving like normal Faculty members, and you\n",
    "  have the opportunity to present\n",
    "\n",
    "        (1) an organized reflection on  the sample class (e.g., addressing\n",
    "        pedagogical issues you considered  when developing this class, the\n",
    "        lecture structure and presentation choices you made and why);\n",
    "\n",
    "        (2) your broader vision of pedagogy;\n",
    "\n",
    "        (3)  how  you  see  yourself  contributing  to  the  departmental,\n",
    "        university, and broader computing communities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Vision \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
