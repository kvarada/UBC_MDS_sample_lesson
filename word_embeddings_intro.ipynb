{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UBC instructor position sample lesson  \n",
    "\n",
    "### By Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "### Today's plan\n",
    "\n",
    "- Set the stage (~5 mins)\n",
    "- Sample class (~40 mins)\n",
    "- Reflection (~10 mins)\n",
    "- Vision (~10 mins)\n",
    "- Q and A (~10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Set the stage (~5 mins)\n",
    "\n",
    "- I am envisioning this as the second lesson of DSCI 575 (Advanced Machine Learning in the context of Natural Language Processing (NLP) applications). \n",
    "- It is the first week of the final block of the MDS program's curriculum. \n",
    "- The students have already taken four 1-credit Machine Learning courses: DSCI 571, DSCI 572, DSCI 573, DSCI 563. \n",
    "- They are now ready to apply the concepts they have learned so far on interesting problems. \n",
    "- The prerequisites I am assuming are\n",
    "    - Familiarity with neural networks, which they have done in DSCI 572."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSCI 575: Advanced Machine Learning (in the context of NLP applications)\n",
    "\n",
    "## Lecture 2: Introduction to Word Embeddings: _Toronto_ : _UofT_ :: _Vancouver_ : ?\n",
    "Instructor: Varada Kolhatkar [ʋəɾəda kɔːlɦəʈkər]\n",
    "\n",
    "\n",
    "#### Today's plan\n",
    "- Quick recap from lecture 1\n",
    "- Word representations\n",
    "    - Sparse representations with co-occurrence matrix\n",
    "    - Dense representation with word2vec (the Skipgram algorithm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "common_texts\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "from preprocessing import MyPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN STARTER CODE\n",
    "class CooccurrenceMatrix:\n",
    "    def __init__(self, corpus, \n",
    "                       tokenizer = word_tokenize, \n",
    "                       window_size = 3):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.window_size = window_size\n",
    "        self.vocab = {}\n",
    "        self.cooccurrence_matrix = None    \n",
    "        \n",
    "    def fit_transform(self):\n",
    "        \"\"\"\n",
    "        Creates a co-occurrence matrix. \n",
    "        \n",
    "        Returns vocabulary (dict) and co-occurrence matrix (csr_matrix)\n",
    "        \"\"\"\n",
    "        data=[]\n",
    "        row=[]\n",
    "        col=[]\n",
    "        for tokens in self.corpus:\n",
    "            for target_index, token in enumerate(tokens):\n",
    "                # Get the index of the word in the vocabulary. If the word is not in the vocabulary, \n",
    "                # set the index to the size of the vocabulary. \n",
    "                i = self.vocab.setdefault(token, len(self.vocab))\n",
    "                \n",
    "                # Consider the context words depending upon the context window \n",
    "                start = max(0, target_index - self.window_size)\n",
    "                end = min(len(tokens), target_index + self.window_size + 1)\n",
    "                \n",
    "                for context_index in range(start, end):\n",
    "                    # Do not consider the target word.  \n",
    "                    if target_index == context_index: \n",
    "                        continue                        \n",
    "                    j = self.vocab.setdefault(tokens[context_index], len(self.vocab))\n",
    "                    # Set diagonal to 0\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    data.append(1.0); row.append(i); col.append(j);\n",
    "        self.cooccurrence_matrix = csr_matrix((data,(row,col)))\n",
    "        return self.vocab, self.cooccurrence_matrix\n",
    "            \n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"\n",
    "        Given a word returns the word vector associated with it from the co-occurrence matrix. \n",
    "\n",
    "        Keyword arguments:\n",
    "        word -- (str) the word to look up in the vocab.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # BEGIN SOLUTION\n",
    "        if word in self.vocab: \n",
    "            return self.cooccurrence_matrix[self.vocab[word]]\n",
    "        else:\n",
    "            print('The word not present in the vocab')\n",
    "        # END SOLUTION\n",
    "\n",
    "# END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ### Last class: What is Natural Language Processing (NLP)?\n",
    "\n",
    "- What would a search engine return when asked the following question? \n",
    "\n",
    "<img src=\"imgs/lexical_ambiguity.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last class: What is Natural Language Processing (NLP)?\n",
    "\n",
    "<img src=\"imgs/WhatisNLP.png\" width=\"900\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is it hard?\n",
    "\n",
    "- Language is complex and subtle. \n",
    "- All the problems related to representation and reasoning in artificial intelligence arise in this domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representing text \n",
    "\n",
    "- Consider a supervised classification task. \n",
    "- So far we have been using X and Y that looks like this: \n",
    "\n",
    "$X = \\begin{bmatrix}1 & 0.8 & 0.3\\\\ 0 & 0 & 0.4\\\\ 1 & 0.2 & 0.8\\\\ \\end{bmatrix}$ and $y = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "- But consider data that looks like this instead: \n",
    "          \n",
    "$X = \\begin{bmatrix}\\text{\"@united you're terrible. You don't understand safety\",}\\\\ \\text{\"@JetBlue safety first !! #lovejetblue\"}\\\\ \\text{\"@SouthwestAir truly the best in #customerservice!\"}\\\\ \\end{bmatrix}$ and $y = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "- ML algorithms we have seen so far prefer well-defined and fixed length input/output and text data is usually messy. \n",
    "- How can we effectively represent these reviews using features? \n",
    "    - Ideally we want to capture the \"meaning\" of text in our representation. \n",
    "- Before moving to meaning of a sentence or a paragraph, let's start with word meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Today's promise\n",
    "\n",
    "- Learn a commonly used robust representation of words.\n",
    "\n",
    "#### Learning outcomes\n",
    "\n",
    "From this class, you will be able to \n",
    "\n",
    "- Explain the general idea of the vector space models and co-occurrence matrices.\n",
    "- Explain the difference between sparse and dense word vectors.\n",
    "- Explain the general idea of the continuous skip-gram model.\n",
    "- Use word embeddings for similarity and analogies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning \n",
    "\n",
    "- A favourite topic of philosophers for centuries. \n",
    "- An example from the legal domain: [Are hockey gloves gloves or \"articles of plastics\"?](https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258)\n",
    "\n",
    "<blockquote>\n",
    "Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either gloves or as \"articles of plastics.\"\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<img src=\"imgs/hockey_gloves_case.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word meaning: NLP view\n",
    "- Modeling word meaning that allows us to \n",
    "    * draw useful inferences to solve meaning-related problems \n",
    "    * find relationship between words, e.g., which words are similar, which ones have positive or negative connotations\n",
    "    * the similarity is usually represented as a dot product between vectors or cosine similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity 1:  Brainstorm ways to represent words (~5 mins) \n",
    "\n",
    "- Suppose you are building a Question Answering system and you are given the following question and three candidate answers. \n",
    "- Discuss the following questions with your neighbour(s). \n",
    "    - What kind of relationship between words do we need to capture in order to arrive at the correct answer?  \n",
    "    - Brainstorm ways to represent words that will allow you to capture this relationship.   \n",
    "<blockquote>       \n",
    "<p style=\"font-size:30px\"><b>Question:</b> How <b>tall</b> is Machu Picchu?</p>\n",
    "    <p style=\"font-size:30px\"><b>Candidate 1:</b> Machu Picchu is 13.164 degrees south of the equator.</p>    \n",
    "<p style=\"font-size:30px\"><b>Candidate 2:</b> The official height of Machu Picchu is 2,430 m.</p>\n",
    "<p style=\"font-size:30px\"><b>Candidate 3:</b> Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.</p>    \n",
    "</blockquote> \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 1: Term-term co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributional hypothesis\n",
    "\n",
    "<blockquote> \n",
    "    <p>You shall know a word by the company it keeps.</p>\n",
    "    <footer>Firth, 1957</footer>        \n",
    "</blockquote>\n",
    "\n",
    "<blockquote> \n",
    "If A and B have almost identical environments we say that they are synonyms.\n",
    "<footer>Harris, 1954</footer>    \n",
    "</blockquote>    \n",
    "\n",
    "Example: \n",
    "\n",
    "- Her **child** loves to play in the playground. \n",
    "- Her **kid** loves to play in the playground. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector space model\n",
    "\n",
    "- A standard way to represent meaning in NLP\n",
    "- Model the meaning of a word by placing it into a vector space.  \n",
    "- Distances among words in the vector space indicate the relationship between them. \n",
    "- Called an \"embedding\" because it's embedded into a high-dimensional space\n",
    "\n",
    "<img src=\"imgs/t-SNE_word_embeddings.png\" width=\"700\" height=\"700\">\n",
    "    (Attribution: Jurafsky and Martin 3rd edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing word vectors and similarity \n",
    "\n",
    "<img src=\"imgs/word_vectors_and_angles.png\" width=\"600\" height=\"600\">\n",
    "(Credit: Jurafsky and Martin 3rd edition)\n",
    "\n",
    "- The similarity is calculated using dot products between word vectors.\n",
    "    - Higher the dot product more similar the words.\n",
    "- We can also calculate a normalized version of dot products. \n",
    "    $$similarity_{cosine}(\\vec{w_1},\\vec{w_2}) = \\frac{\\vec{w_1}.\\vec{w_2}}{\\left\\lVert w_1\\right\\rVert_2 \\left\\lVert w_2\\right\\rVert_2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"How tall is Machu Picchu?\",\n",
    "          \"Machu Picchu is 13.164 degrees south of the equator.\", \n",
    "          \"The official height of Machu Picchu is 2,430 m.\",\n",
    "          \"Machu Picchu is 80 kilometres (50 miles) northwest of Cusco.\",\n",
    "          \"It is 80 kilometres (50 miles) northwest of Cusco, on the crest of the mountain Machu Picchu, located about 2,430 metres (7,970 feet) above mean sea level, over 1,000 metres (3,300 ft) lower than Cusco, which has an elevation of 3,400 metres (11,200 ft).\"\n",
    "         ]\n",
    "pp = MyPreprocessor()\n",
    "pp_corpus = pp.preprocess_corpus(corpus)\n",
    "vec = CountVectorizer(tokenizer = nltk.word_tokenize)\n",
    "X = vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tall</th>\n",
       "      <th>machu</th>\n",
       "      <th>picchu</th>\n",
       "      <th>13.164</th>\n",
       "      <th>degrees</th>\n",
       "      <th>south</th>\n",
       "      <th>equator</th>\n",
       "      <th>official</th>\n",
       "      <th>height</th>\n",
       "      <th>2,430</th>\n",
       "      <th>80</th>\n",
       "      <th>kilometres</th>\n",
       "      <th>50</th>\n",
       "      <th>miles</th>\n",
       "      <th>northwest</th>\n",
       "      <th>cusco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tall</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machu</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picchu</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.164</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tall  machu  picchu  13.164  degrees  south  equator  official  \\\n",
       "tall        0      1       1       0        0      0        0         0   \n",
       "machu       1      0       4       1        1      0        0         1   \n",
       "picchu      1      4       0       1        1      1        0         1   \n",
       "13.164      0      1       1       0        1      1        1         0   \n",
       "degrees     0      1       1       1        0      1        1         0   \n",
       "\n",
       "         height  2,430  80  kilometres  50  miles  northwest  cusco  \n",
       "tall          0      0   0           0   0      0          0      0  \n",
       "machu         1      1   1           1   0      0          0      0  \n",
       "picchu        1      1   1           1   1      0          0      0  \n",
       "13.164        0      0   0           0   0      0          0      0  \n",
       "degrees       0      0   0           0   0      0          0      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CooccurrenceMatrix(pp_corpus)\n",
    "vocab, comat = cm.fit_transform()\n",
    "words = [key for key, value in sorted(vocab.items(), \n",
    "                                      key = lambda item: (item[1],item[0]))]\n",
    "df = pd.DataFrame(comat.todense(), \n",
    "                  columns = words, \n",
    "                  index = words,\n",
    "                  dtype = np.int8\n",
    "                 )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product between tall and height is 2.00 and cosine similarity is 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vec1 = cm.get_word_vector('tall').todense().flatten()\n",
    "vec2 = cm.get_word_vector('height').todense().flatten()\n",
    "v1 = np.squeeze(np.asarray(vec1))\n",
    "v2 = np.squeeze(np.asarray(vec2))\n",
    "print('The dot product between tall and height is %0.2f and cosine similarity is %0.2f' \n",
    "      %(v1.dot(v2),cosine_similarity(vec1, vec2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse vs. dense word vectors\n",
    "\n",
    "- Term-term co-occurrence matrices are long and sparse. \n",
    "    - length |V|= 20,000 to 50,000\n",
    "    - most elements are zero\n",
    "- OK because there are efficient ways to deal with sparse matrices.\n",
    "\n",
    "\n",
    "### Alternative \n",
    "- Learn short (~100 to 1000 dimensions) and dense vectors. \n",
    "- Short vectors may be easier to train with ML models (less weights to train).\n",
    "- They may generalize better.\n",
    "- In practice they work much better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation 2: Dense word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of algorithms to create dense word embeddings\n",
    "- $|V| \\rightarrow$ vocabulary size\n",
    "- $d \\rightarrow$ number of dimensions \n",
    "<img src=\"imgs/word2vec.png\" width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Activity 2: Try out Word similarity with word embeddings (~4 mins)\n",
    "\n",
    "- Go to the following online demo of word embeddings created by Turku NLP group from Finland.\n",
    "http://bionlp-www.utu.fi/wv_demo/\n",
    "- For `Select one of the available models` select `English GoogleNews Negative300`\n",
    "- For `Nearest words` option, type a word and get the most similar words for the given word. Some suggestions are:\n",
    "    - UBC\n",
    "    - smart\n",
    "    \n",
    "- To get the similarity between two words, under `Similarity of two words`, type word pairs of your interest. Some options are: \n",
    "    - **tall** and **height** \n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### How can we get dense vectors?\n",
    " \n",
    "- Count-based methods\n",
    "    - Singular Value Decomposition (SVD)\n",
    "- Prediction-based methods\n",
    "    - [Word2Vec](https://github.com/tmikolov/word2vec)\n",
    "    - [fastText](https://fasttext.cc/)\n",
    "    - [GloVe](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec \n",
    "\n",
    "- A family of models to get dense word vectors.\n",
    "\n",
    "- Two primary algorithms \n",
    "    - **Skip-gram**\n",
    "    - Continuous bag of words\n",
    "\n",
    "- Two moderately efficient training methods \n",
    "    - Hierarchical softmax\n",
    "    - negative sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fake word-prediction task \n",
    "\n",
    "- Given a target word, predict context words. \n",
    "<center>\n",
    "<blockquote>\n",
    "    Add freshly squeezed$_{context}$ pineapple$_{target}$ juice$_{context}$ to your smoothie. \n",
    "</blockquote> \n",
    "</center>\n",
    "\n",
    "- Target word: **pineapple**\n",
    "- Given a *target* word, we define a **context window** and predict each word in the given context window.\n",
    "- If the context window is $n$, consider $n$ preceding and $n$ following words of the target word.\n",
    "- In the above the context window is 1. \n",
    "- Then our fake task is predicting whether **juice** and **squeezed** are likely to occur in the context of **pineapple**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous skip-gram objective\n",
    "- Consider the conditional probabilities $p(w_c|w_t)$ and set the parameters $\\theta$ of $p(w_c|w_t; \\theta)$ so as to maximize the corpus probability. \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} p(w_c|w_t;\\theta)\n",
    "$\n",
    "</center>\n",
    "\n",
    "- $w_t$ &rarr; target word, \n",
    "- $m$ &rarr; the context window size\n",
    "- $D$ is the set of all word and context pairs from the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous skip-gram objective\n",
    "\n",
    "- Model the conditional probability using softmax of the dot product.\n",
    "    * Higher the dot product higher the probability and vice-versa.     \n",
    "    \n",
    "\n",
    "$$P(w_c|w_t;\\theta) = \\frac{exp(\\vec{w_c}.\\vec{w_t})}{\\sum\\limits_{\\substack{c' \\in V}} exp(\\vec{w_{c'}}.\\vec{w_t})}\\\\\n",
    "$$\n",
    "\n",
    "- Substituting the conditional probability with the softmax of dot product: \n",
    "$$    \n",
    "\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} p(w_c|w_t;\\theta) \\approx \\prod\\limits_{(w_c,w_t) \\in D}\\frac{exp(\\vec{w_c}.\\vec{w_t})}{\\sum\\limits_{\\substack{c' \\in V}} exp(\\vec{w_{c'}}.\\vec{w_t})}$$\n",
    "- Assumption: Maximizing this objective will results in meaningful embeddings for all words in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/skip-gram.png\" width=\"1000\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters to learn\n",
    "\n",
    "- Given a corpus with vocabulary of size $V$, where a word $w_i$ is identified by its index $i \\in {1, ..., V}$, learn a vector representation for each $w_i$ by predicting the words that appear in its context. \n",
    "- Learn the following parameters of the model\n",
    "    - Suppose $V = 10,000$, $d = 300$, the number of parameters to learn are 6,000,000! \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\theta = \n",
    "\\begin{bmatrix} aardvark_t\\\\\n",
    "                aback_t\\\\\n",
    "                \\dots\\\\\n",
    "                zymurgi_t\\\\\n",
    "                aardvark_c\\\\\n",
    "                aback_c\\\\                \n",
    "                \\dots\\\\\n",
    "                zymurgi_c\\\\                \n",
    "\\end{bmatrix} \\in R^{2dV}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main hyperparameters of the model\n",
    "\n",
    "- Dimensionality of the word vectors \n",
    "- Window size\n",
    "    * shorter window: more syntactic representation\n",
    "    * longer window: more semantic representation \n",
    "    * Mikolov et al. (2015) suggest setting this parameter in the range 5 to 20 for small training datasets and in the range 2 to 5 for large training datasets.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training word2vec embeddings \n",
    "\n",
    "- [Original C code](https://code.google.com/archive/p/word2vec/) \n",
    "- [GitHub version of the code](https://github.com/tmikolov/word2vec)\n",
    "- [Gensim](https://radimrehurek.com/gensim/), an open source Python library has provides a Python interface for word2vec family of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.76157013e-03, -2.89657875e-03,  3.76423285e-03,  5.90864875e-05,\n",
       "        3.47665837e-03, -2.62535817e-04, -3.32250464e-04, -4.19511553e-03,\n",
       "       -4.36926668e-04,  1.56240480e-03, -3.05232243e-03, -4.54633636e-03,\n",
       "        4.06075222e-03,  1.75776985e-03,  3.43788974e-03,  2.02650996e-03,\n",
       "       -4.95262025e-03,  3.03766388e-03, -1.21213193e-03,  4.29645600e-03,\n",
       "        3.24825034e-03,  1.96010550e-03, -2.42687156e-03,  4.59062494e-03,\n",
       "       -1.69572595e-03,  4.59186453e-03,  4.29759687e-03,  1.95399369e-03,\n",
       "        3.22686741e-03, -1.77241932e-03, -4.07945656e-04, -3.50145600e-03,\n",
       "       -2.19744956e-03, -2.72836653e-03,  2.99455621e-03,  2.31481517e-05,\n",
       "        1.33064261e-03,  2.95168976e-03,  1.87403732e-03, -2.45945551e-03,\n",
       "       -7.19729636e-04, -1.93787715e-03,  2.95945723e-03,  1.32702431e-03,\n",
       "       -3.25158844e-03,  1.55024079e-03,  4.01399564e-03,  3.95181868e-03,\n",
       "        3.57035990e-03,  4.86840634e-03, -3.14707262e-03,  4.10378492e-03,\n",
       "        1.25811424e-03, -6.87815947e-04,  3.60252033e-03,  1.76963012e-03,\n",
       "       -3.17253429e-03,  3.28388717e-03,  1.48962880e-03,  1.10192667e-03,\n",
       "       -4.97768447e-03, -3.47654498e-03,  2.15311139e-03,  2.55706185e-03,\n",
       "        1.70701393e-03,  2.81170127e-03,  2.42012134e-03, -1.42664427e-03,\n",
       "        4.99170134e-03, -2.42253393e-03, -2.04711556e-04,  3.65570001e-03,\n",
       "        2.53789825e-03, -2.13201065e-03,  3.17509146e-03,  1.64631358e-03,\n",
       "        3.24176089e-03,  3.47093586e-03, -2.50747730e-03, -4.10106499e-03,\n",
       "       -2.90237111e-03, -6.17225887e-04, -3.37505131e-03,  6.49042369e-04,\n",
       "       -3.24948435e-03, -3.00741591e-03, -1.79497263e-04,  1.08772096e-04,\n",
       "       -2.44843494e-03, -3.82240070e-03,  1.68269651e-03,  1.40291383e-03,\n",
       "        2.21411092e-03,  3.78780998e-03,  4.62130783e-03, -2.80050421e-03,\n",
       "       -4.78748139e-03,  2.52238265e-03, -2.57050339e-03, -3.97718791e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(common_texts, \n",
    "                 size=100, \n",
    "                 window=5, \n",
    "                 min_count=1,\n",
    "                 sg=1, \n",
    "                 negative=4, \n",
    "                )\n",
    "model.wv['trees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other popular methods to get embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained embeddings\n",
    "\n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "    * trained on several corpora using the word2vec algorithm \n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using [the fastText algorithm](http://aclweb.org/anthology/Q17-1010)\n",
    "    * published by Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  3000000\n",
      "The similarity between height and tall is 0.473\n",
      "The similarity between pineapple and mango is 0.668\n",
      "The similarity between pineapple and juice is 0.418\n",
      "The similarity between sun and robot is 0.029\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary: ', len(model.vocab))\n",
    "word_pairs = [('height','tall'),\n",
    "              ('pineapple','mango'), \n",
    "              ('pineapple','juice'), \n",
    "              ('sun','robot')]\n",
    "for pair in word_pairs: \n",
    "    print('The similarity between %s and %s is %0.3f' %(pair[0], pair[1], model.similarity(pair[0], pair[1])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Success of Word2Vec\n",
    "\n",
    "- Able to capture complex relationships between words.\n",
    "- Example: What is the word that is similar to **WOMAN** in the same sense as **KING** is similar to **MAN**?\n",
    "- Perform a simple algebraic operations with the vector representation of words.\n",
    "    $\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance.\n",
    "\n",
    "<img src=\"files/images/word_analogies1.png\" width=\"500\" height=\"500\">\n",
    "(Credit: Mikolov et al. 2013)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model = model):\n",
    "    '''    \n",
    "    Returns analogy word using the given model. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    word1 -- (str) \n",
    "    word2 -- (str)\n",
    "    word3 -- (str)\n",
    "    model -- word embedding model\n",
    "    '''\n",
    "    print('%s : %s :: %s : ?' %(word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=['Analogy word', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.711819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monarch</td>\n",
       "      <td>0.618967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.590243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crown_prince</td>\n",
       "      <td>0.549946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prince</td>\n",
       "      <td>0.537732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kings</td>\n",
       "      <td>0.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Queen_Consort</td>\n",
       "      <td>0.523595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queens</td>\n",
       "      <td>0.518113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sultan</td>\n",
       "      <td>0.509859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>monarchy</td>\n",
       "      <td>0.508741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0          queen  0.711819\n",
       "1        monarch  0.618967\n",
       "2       princess  0.590243\n",
       "3   crown_prince  0.549946\n",
       "4         prince  0.537732\n",
       "5          kings  0.523684\n",
       "6  Queen_Consort  0.523595\n",
       "7         queens  0.518113\n",
       "8         sultan  0.509859\n",
       "9       monarchy  0.508741"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man','king','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montreal : Canadiens :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canucks</td>\n",
       "      <td>0.821327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vancouver_Canucks</td>\n",
       "      <td>0.750401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary_Flames</td>\n",
       "      <td>0.705470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leafs</td>\n",
       "      <td>0.695783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maple_Leafs</td>\n",
       "      <td>0.691617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thrashers</td>\n",
       "      <td>0.687504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avs</td>\n",
       "      <td>0.681716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sabres</td>\n",
       "      <td>0.665307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Blackhawks</td>\n",
       "      <td>0.664625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Habs</td>\n",
       "      <td>0.661023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Analogy word     Score\n",
       "0            Canucks  0.821327\n",
       "1  Vancouver_Canucks  0.750401\n",
       "2     Calgary_Flames  0.705470\n",
       "3              Leafs  0.695783\n",
       "4        Maple_Leafs  0.691617\n",
       "5          Thrashers  0.687504\n",
       "6                Avs  0.681716\n",
       "7             Sabres  0.665307\n",
       "8         Blackhawks  0.664625\n",
       "9               Habs  0.661023"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Montreal', 'Canadiens', 'Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto : UofT :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SFU</td>\n",
       "      <td>0.579245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UVic</td>\n",
       "      <td>0.576921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UBC</td>\n",
       "      <td>0.571431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simon_Fraser</td>\n",
       "      <td>0.543464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Langara_College</td>\n",
       "      <td>0.541347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UVIC</td>\n",
       "      <td>0.520495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grant_MacEwan</td>\n",
       "      <td>0.517273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UFV</td>\n",
       "      <td>0.514150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ubyssey</td>\n",
       "      <td>0.510421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kwantlen</td>\n",
       "      <td>0.503807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Analogy word     Score\n",
       "0              SFU  0.579245\n",
       "1             UVic  0.576921\n",
       "2              UBC  0.571431\n",
       "3     Simon_Fraser  0.543464\n",
       "4  Langara_College  0.541347\n",
       "5             UVIC  0.520495\n",
       "6    Grant_MacEwan  0.517273\n",
       "7              UFV  0.514150\n",
       "8          Ubyssey  0.510421\n",
       "9         Kwantlen  0.503807"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('Toronto', 'UofT', 'Vancouver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implicit biases and stereotypes in word embeddings\n",
    "\n",
    "- Reflect gender stereotypes present in broader society.\n",
    "- They may also amplify these stereotypes because of their widespread usage. \n",
    "- See [this paper](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : computer_programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similar word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.562712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.510505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphic_designer</td>\n",
       "      <td>0.505180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.497949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.493489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.492551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>0.490797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saleswoman</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electrical_engineer</td>\n",
       "      <td>0.479773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanical_engineer</td>\n",
       "      <td>0.475540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Similar word     Score\n",
       "0            homemaker  0.562712\n",
       "1            housewife  0.510505\n",
       "2     graphic_designer  0.505180\n",
       "3        schoolteacher  0.497949\n",
       "4        businesswoman  0.493489\n",
       "5            paralegal  0.492551\n",
       "6     registered_nurse  0.490797\n",
       "7           saleswoman  0.488163\n",
       "8  electrical_engineer  0.479773\n",
       "9  mechanical_engineer  0.475540"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'computer_programmer', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity: Explore analogies with word embeddings (5 mins)\n",
    "\n",
    "- Again go to following online demo. \n",
    "http://bionlp-www.utu.fi/wv_demo/ \n",
    "- Team up with your neighbour and try out some pairs for analogies together (~3 minutes).\n",
    "- Class discussion (~2 minutes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Vector space model \n",
    "    * Modeling word meaning by placing it in a vector space.\n",
    "    * Distance between words in this vector space indicate the relationship between them. \n",
    "- Word embeddings\n",
    "    * Creating short and dense representations of words. \n",
    "- Word2Vec\n",
    "    * A family of models to learn dense vector representations of words\n",
    "    * Freely available code and pre-trained models \n",
    "    * Available for many different languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up \n",
    "\n",
    "- Wait! Don't we want to represent sentences and paragraphs for tasks such as below? \n",
    "          \n",
    "$X = \\begin{bmatrix}\\text{\"@united you're terrible. You don't understand safety\",}\\\\ \\text{\"@JetBlue safety first !! #lovejetblue\"}\\\\ \\text{\"@SouthwestAir truly the best in #customerservice!\"}\\\\ \\end{bmatrix}$ and $y = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "- We will build on the idea of word embeddings to come up with general text representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonus: Relevant papers\n",
    "\n",
    "- [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [Efficient estimation of word representations in vector space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Linguistic regularities in continuous space word representations](https://www.aclweb.org/anthology/N13-1090)\n",
    "- [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Bonus: Examples of semantic and syntactic relationships\n",
    "\n",
    "<center>\n",
    "<img src=\"files/images/word_analogies2.png\" width=\"800\" height=\"800\">\n",
    "(Credit: Mikolov 2013)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonus: Links for pre-trained embeddings\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "\n",
    "### Bonus: Fun tools\n",
    "[wevi: word embedding visual inspector](https://ronxin.github.io/wevi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reflection \n",
    "\n",
    "- TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vision \n",
    "\n",
    "- TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
